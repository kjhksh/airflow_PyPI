[2025-04-18T12:47:54.430+0900] {executor_loader.py:258} INFO - Loaded executor: LocalExecutor
[2025-04-18T12:47:59.500+0900] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-04-18T12:47:59.501+0900] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-04-18T12:47:59.627+0900] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 673376
[2025-04-18T12:47:59.629+0900] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-18T12:47:59.636+0900] {settings.py:63} INFO - Configured default timezone Asia/Seoul
[2025-04-18T12:52:59.996+0900] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-04-18T12:53:59.631+0900] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
[2025-04-18T12:53:59.632+0900] {scheduler_job_runner.py:507} INFO - DAG dag_salesforce_to_s3_python_improved has 0/16 running and queued tasks
[2025-04-18T12:53:59.632+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
[2025-04-18T12:53:59.636+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:53:59.636+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 1 and queue default
[2025-04-18T12:53:59.637+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
[2025-04-18T12:53:59.698+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
[2025-04-18T12:53:59.775+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_salesforce_to_s3_python_improved.py
[2025-04-18T12:54:00.703+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:54:05.631+0900] {dagrun.py:854} INFO - Marking run <DagRun dag_salesforce_to_s3_python_improved @ 2025-04-18 03:53:59.132483+00:00: manual__2025-04-18T03:53:59.132483+00:00, state:running, queued_at: 2025-04-18 03:53:59.170129+00:00. externally triggered: True> successful
[2025-04-18T12:54:05.632+0900] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_salesforce_to_s3_python_improved, execution_date=2025-04-18 03:53:59.132483+00:00, run_id=manual__2025-04-18T03:53:59.132483+00:00, run_start_date=2025-04-18 03:53:59.512868+00:00, run_end_date=2025-04-18 03:54:05.632608+00:00, run_duration=6.11974, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-18 03:53:59.132483+00:00, data_interval_end=2025-04-18 03:53:59.132483+00:00, dag_hash=a4fa51db3e4e05d07994e5ebb405e51c
[2025-04-18T12:54:05.698+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=1, map_index=-1)
[2025-04-18T12:54:05.706+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_salesforce_to_s3_python_improved, task_id=export_salesforce_to_s3_task, run_id=manual__2025-04-18T03:53:59.132483+00:00, map_index=-1, run_start_date=2025-04-18 03:54:00.992479+00:00, run_end_date=2025-04-18 03:54:04.445926+00:00, run_duration=3.453447, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-04-18 03:53:59.633407+00:00, queued_by_job_id=6, pid=675569
[2025-04-18T12:54:31.078+0900] {dag.py:4180} INFO - Setting next_dagrun for dag_oracle_to_s3_python_improved to 2025-04-17 15:05:00+00:00, run_after=2025-04-18 15:05:00+00:00
[2025-04-18T12:54:31.444+0900] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:54:31.445+0900] {scheduler_job_runner.py:507} INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
[2025-04-18T12:54:31.445+0900] {scheduler_job_runner.py:507} INFO - DAG dag_oracle_to_s3_python_improved has 1/16 running and queued tasks
[2025-04-18T12:54:31.445+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:54:31.448+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>, <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:54:31.449+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 2 and queue default
[2025-04-18T12:54:31.449+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:31.449+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 2 and queue default
[2025-04-18T12:54:31.449+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:31.492+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:31.493+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:31.542+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
[2025-04-18T12:54:31.581+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
[2025-04-18T12:54:32.304+0900] {dag_oracle_to_s3_python_improved.py:25} INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
[2025-04-18T12:54:32.305+0900] {dag_oracle_to_s3_python_improved.py:26} INFO - project name : dag_oracle_to_s3_python_improved
[2025-04-18T12:54:32.305+0900] {dag_oracle_to_s3_python_improved.py:27} INFO - project version : 0.1.0
[2025-04-18T12:54:32.305+0900] {dag_oracle_to_s3_python_improved.py:28} INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
[2025-04-18T12:54:32.306+0900] {dag_oracle_to_s3_python_improved.py:29} INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
[2025-04-18T12:54:32.306+0900] {dag_oracle_to_s3_python_improved.py:30} INFO - project start : 2025-04-18 12:54:32
[2025-04-18T12:54:32.320+0900] {dag_oracle_to_s3_python_improved.py:25} INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
[2025-04-18T12:54:32.321+0900] {dag_oracle_to_s3_python_improved.py:26} INFO - project name : dag_oracle_to_s3_python_improved
[2025-04-18T12:54:32.321+0900] {dag_oracle_to_s3_python_improved.py:27} INFO - project version : 0.1.0
[2025-04-18T12:54:32.322+0900] {dag_oracle_to_s3_python_improved.py:28} INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
[2025-04-18T12:54:32.322+0900] {dag_oracle_to_s3_python_improved.py:29} INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
[2025-04-18T12:54:32.322+0900] {dag_oracle_to_s3_python_improved.py:30} INFO - project start : 2025-04-18 12:54:32
[2025-04-18T12:54:32.347+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:54:32.367+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:54:52.123+0900] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
[2025-04-18T12:54:52.124+0900] {scheduler_job_runner.py:507} INFO - DAG dag_oracle_to_s3_python_improved has 1/16 running and queued tasks
[2025-04-18T12:54:52.124+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
[2025-04-18T12:54:52.126+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:54:52.127+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 1 and queue default
[2025-04-18T12:54:52.127+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:52.145+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1)
[2025-04-18T12:54:52.145+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:52.150+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=oralceExport_task, run_id=scheduled__2025-04-16T15:05:00+00:00, map_index=-1, run_start_date=2025-04-18 03:54:32.534117+00:00, run_end_date=2025-04-18 03:54:51.757934+00:00, run_duration=19.223817, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-04-18 03:54:31.446476+00:00, queued_by_job_id=6, pid=675838
[2025-04-18T12:54:52.279+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
[2025-04-18T12:54:53.000+0900] {dag_oracle_to_s3_python_improved.py:25} INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
[2025-04-18T12:54:53.001+0900] {dag_oracle_to_s3_python_improved.py:26} INFO - project name : dag_oracle_to_s3_python_improved
[2025-04-18T12:54:53.002+0900] {dag_oracle_to_s3_python_improved.py:27} INFO - project version : 0.1.0
[2025-04-18T12:54:53.002+0900] {dag_oracle_to_s3_python_improved.py:28} INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
[2025-04-18T12:54:53.003+0900] {dag_oracle_to_s3_python_improved.py:29} INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
[2025-04-18T12:54:53.003+0900] {dag_oracle_to_s3_python_improved.py:30} INFO - project start : 2025-04-18 12:54:53
[2025-04-18T12:54:53.051+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:54:53.621+0900] {dagrun.py:854} INFO - Marking run <DagRun dag_oracle_to_s3_python_improved @ 2025-04-16 15:05:00+00:00: scheduled__2025-04-16T15:05:00+00:00, state:running, queued_at: 2025-04-18 03:54:31.064965+00:00. externally triggered: False> successful
[2025-04-18T12:54:53.621+0900] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_oracle_to_s3_python_improved, execution_date=2025-04-16 15:05:00+00:00, run_id=scheduled__2025-04-16T15:05:00+00:00, run_start_date=2025-04-18 03:54:31.099665+00:00, run_end_date=2025-04-18 03:54:53.621738+00:00, run_duration=22.522073, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-04-16 15:05:00+00:00, data_interval_end=2025-04-17 15:05:00+00:00, dag_hash=1468a60cd25d7fdf91c8485574c935b5
[2025-04-18T12:54:53.629+0900] {dag.py:4180} INFO - Setting next_dagrun for dag_oracle_to_s3_python_improved to 2025-04-17 15:05:00+00:00, run_after=2025-04-18 15:05:00+00:00
[2025-04-18T12:54:53.753+0900] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:54:53.754+0900] {scheduler_job_runner.py:507} INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
[2025-04-18T12:54:53.755+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:54:53.758+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:54:53.759+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 1 and queue default
[2025-04-18T12:54:53.759+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:53.773+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1)
[2025-04-18T12:54:53.773+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1)
[2025-04-18T12:54:53.773+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:54:53.782+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=oralceExport_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:54:32.498437+00:00, run_end_date=2025-04-18 03:54:52.828031+00:00, run_duration=20.329594, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-04-18 03:54:31.446476+00:00, queued_by_job_id=6, pid=675836
[2025-04-18T12:54:53.783+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=cleanSpace_task, run_id=scheduled__2025-04-16T15:05:00+00:00, map_index=-1, run_start_date=2025-04-18 03:54:53.153089+00:00, run_end_date=2025-04-18 03:54:53.440555+00:00, run_duration=0.287466, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-18 03:54:52.125151+00:00, queued_by_job_id=6, pid=676021
[2025-04-18T12:54:53.876+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
[2025-04-18T12:54:54.572+0900] {dag_oracle_to_s3_python_improved.py:25} INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
[2025-04-18T12:54:54.573+0900] {dag_oracle_to_s3_python_improved.py:26} INFO - project name : dag_oracle_to_s3_python_improved
[2025-04-18T12:54:54.573+0900] {dag_oracle_to_s3_python_improved.py:27} INFO - project version : 0.1.0
[2025-04-18T12:54:54.573+0900] {dag_oracle_to_s3_python_improved.py:28} INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
[2025-04-18T12:54:54.573+0900] {dag_oracle_to_s3_python_improved.py:29} INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
[2025-04-18T12:54:54.574+0900] {dag_oracle_to_s3_python_improved.py:30} INFO - project start : 2025-04-18 12:54:54
[2025-04-18T12:54:54.616+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:54:56.053+0900] {dagrun.py:854} INFO - Marking run <DagRun dag_oracle_to_s3_python_improved @ 2025-04-18 03:54:30.166214+00:00: manual__2025-04-18T03:54:30.166214+00:00, state:running, queued_at: 2025-04-18 03:54:30.206488+00:00. externally triggered: True> successful
[2025-04-18T12:54:56.054+0900] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_oracle_to_s3_python_improved, execution_date=2025-04-18 03:54:30.166214+00:00, run_id=manual__2025-04-18T03:54:30.166214+00:00, run_start_date=2025-04-18 03:54:31.100184+00:00, run_end_date=2025-04-18 03:54:56.054231+00:00, run_duration=24.954047, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-16 15:05:00+00:00, data_interval_end=2025-04-17 15:05:00+00:00, dag_hash=1468a60cd25d7fdf91c8485574c935b5
[2025-04-18T12:54:56.110+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1)
[2025-04-18T12:54:56.116+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=cleanSpace_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:54:54.707455+00:00, run_end_date=2025-04-18 03:54:55.031334+00:00, run_duration=0.323879, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-18 03:54:53.756307+00:00, queued_by_job_id=6, pid=676048
[2025-04-18T12:55:29.578+0900] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
[2025-04-18T12:55:29.578+0900] {scheduler_job_runner.py:507} INFO - DAG dag_salesforce_to_s3_python_improved has 0/16 running and queued tasks
[2025-04-18T12:55:29.579+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
[2025-04-18T12:55:29.581+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:55:29.581+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=2, map_index=-1) to LocalExecutor with priority 1 and queue default
[2025-04-18T12:55:29.581+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
[2025-04-18T12:55:29.585+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
[2025-04-18T12:55:29.617+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_salesforce_to_s3_python_improved.py
[2025-04-18T12:55:30.434+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:55:33.844+0900] {dagrun.py:854} INFO - Marking run <DagRun dag_salesforce_to_s3_python_improved @ 2025-04-18 03:53:59.132483+00:00: manual__2025-04-18T03:53:59.132483+00:00, state:running, queued_at: 2025-04-18 03:55:28.598355+00:00. externally triggered: True> successful
[2025-04-18T12:55:33.845+0900] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_salesforce_to_s3_python_improved, execution_date=2025-04-18 03:53:59.132483+00:00, run_id=manual__2025-04-18T03:53:59.132483+00:00, run_start_date=2025-04-18 03:55:29.548043+00:00, run_end_date=2025-04-18 03:55:33.845339+00:00, run_duration=4.297296, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-18 03:53:59.132483+00:00, data_interval_end=2025-04-18 03:53:59.132483+00:00, dag_hash=a4fa51db3e4e05d07994e5ebb405e51c
[2025-04-18T12:55:33.859+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=2, map_index=-1)
[2025-04-18T12:55:33.864+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_salesforce_to_s3_python_improved, task_id=export_salesforce_to_s3_task, run_id=manual__2025-04-18T03:53:59.132483+00:00, map_index=-1, run_start_date=2025-04-18 03:55:30.531349+00:00, run_end_date=2025-04-18 03:55:33.361262+00:00, run_duration=2.829913, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-04-18 03:55:29.579751+00:00, queued_by_job_id=6, pid=676307
[2025-04-18T12:55:40.592+0900] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:55:40.592+0900] {scheduler_job_runner.py:507} INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
[2025-04-18T12:55:40.592+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:55:40.594+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:55:40.595+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1) to LocalExecutor with priority 2 and queue default
[2025-04-18T12:55:40.595+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:55:40.599+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:55:40.627+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
[2025-04-18T12:55:41.248+0900] {dag_oracle_to_s3_python_improved.py:25} INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
[2025-04-18T12:55:41.249+0900] {dag_oracle_to_s3_python_improved.py:26} INFO - project name : dag_oracle_to_s3_python_improved
[2025-04-18T12:55:41.249+0900] {dag_oracle_to_s3_python_improved.py:27} INFO - project version : 0.1.0
[2025-04-18T12:55:41.250+0900] {dag_oracle_to_s3_python_improved.py:28} INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
[2025-04-18T12:55:41.250+0900] {dag_oracle_to_s3_python_improved.py:29} INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
[2025-04-18T12:55:41.251+0900] {dag_oracle_to_s3_python_improved.py:30} INFO - project start : 2025-04-18 12:55:41
[2025-04-18T12:55:41.297+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:55:52.924+0900] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:55:52.924+0900] {scheduler_job_runner.py:507} INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
[2025-04-18T12:55:52.925+0900] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
[2025-04-18T12:55:52.927+0900] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
[2025-04-18T12:55:52.927+0900] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1) to LocalExecutor with priority 1 and queue default
[2025-04-18T12:55:52.928+0900] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:55:52.931+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1)
[2025-04-18T12:55:52.932+0900] {local_executor.py:93} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
[2025-04-18T12:55:52.937+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=oralceExport_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:55:41.389328+00:00, run_end_date=2025-04-18 03:55:52.563060+00:00, run_duration=11.173732, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-04-18 03:55:40.593298+00:00, queued_by_job_id=6, pid=676396
[2025-04-18T12:55:52.963+0900] {dagbag.py:588} INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
[2025-04-18T12:55:53.709+0900] {dag_oracle_to_s3_python_improved.py:25} INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
[2025-04-18T12:55:53.710+0900] {dag_oracle_to_s3_python_improved.py:26} INFO - project name : dag_oracle_to_s3_python_improved
[2025-04-18T12:55:53.710+0900] {dag_oracle_to_s3_python_improved.py:27} INFO - project version : 0.1.0
[2025-04-18T12:55:53.710+0900] {dag_oracle_to_s3_python_improved.py:28} INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
[2025-04-18T12:55:53.711+0900] {dag_oracle_to_s3_python_improved.py:29} INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
[2025-04-18T12:55:53.711+0900] {dag_oracle_to_s3_python_improved.py:30} INFO - project start : 2025-04-18 12:55:53
[2025-04-18T12:55:53.753+0900] {task_command.py:467} INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
[2025-04-18T12:55:55.100+0900] {dagrun.py:854} INFO - Marking run <DagRun dag_oracle_to_s3_python_improved @ 2025-04-18 03:54:30.166214+00:00: manual__2025-04-18T03:54:30.166214+00:00, state:running, queued_at: 2025-04-18 03:55:38.701679+00:00. externally triggered: True> successful
[2025-04-18T12:55:55.100+0900] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_oracle_to_s3_python_improved, execution_date=2025-04-18 03:54:30.166214+00:00, run_id=manual__2025-04-18T03:54:30.166214+00:00, run_start_date=2025-04-18 03:55:39.687016+00:00, run_end_date=2025-04-18 03:55:55.100801+00:00, run_duration=15.413785, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-16 15:05:00+00:00, data_interval_end=2025-04-17 15:05:00+00:00, dag_hash=1468a60cd25d7fdf91c8485574c935b5
[2025-04-18T12:55:55.116+0900] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1)
[2025-04-18T12:55:55.120+0900] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=cleanSpace_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:55:53.836780+00:00, run_end_date=2025-04-18 03:55:54.042132+00:00, run_duration=0.205352, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-18 03:55:52.925594+00:00, queued_by_job_id=6, pid=676532
[2025-04-18T12:58:00.365+0900] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
