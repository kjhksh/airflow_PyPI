2025-04-18 11:53:25,770 WARNING - Could not import pandas. Holidays will not be considered.
2025-04-18 11:53:25,772 INFO - Loaded executor: LocalExecutor
2025-04-18 12:05:11,285 WARNING - Could not import pandas. Holidays will not be considered.
2025-04-18 12:05:11,288 INFO - Loaded executor: LocalExecutor
2025-04-18 12:05:16,341 INFO - Starting the scheduler
2025-04-18 12:05:16,342 INFO - Processing each file at most -1 times
2025-04-18 12:05:16,423 INFO - Launched DagFileProcessorManager with pid: 653472
2025-04-18 12:05:16,426 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:05:16,445 INFO - Configured default timezone Asia/Seoul
2025-04-18 12:06:37,314 INFO - Exiting gracefully upon receiving signal 15
2025-04-18 12:06:37,391 INFO - Sending Signals.SIGTERM to group 653472. PIDs of all processes in the group: []
2025-04-18 12:06:37,391 INFO - Sending the signal Signals.SIGTERM to group 653472
2025-04-18 12:06:37,391 INFO - Sending the signal Signals.SIGTERM to process 653472 as process group is missing.
2025-04-18 12:06:37,392 INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
2025-04-18 12:06:37,445 INFO - Sending Signals.SIGTERM to group 653472. PIDs of all processes in the group: []
2025-04-18 12:06:37,447 INFO - Sending the signal Signals.SIGTERM to group 653472
2025-04-18 12:06:37,447 INFO - Sending the signal Signals.SIGTERM to process 653472 as process group is missing.
2025-04-18 12:06:37,447 INFO - Exited execute loop
2025-04-18 12:12:05,205 INFO - Loaded executor: LocalExecutor
2025-04-18 12:12:10,258 INFO - Starting the scheduler
2025-04-18 12:12:10,258 INFO - Processing each file at most -1 times
2025-04-18 12:12:10,396 INFO - Launched DagFileProcessorManager with pid: 656419
2025-04-18 12:12:10,399 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:12:10,404 INFO - Configured default timezone Asia/Seoul
2025-04-18 12:14:01,499 INFO - Exiting gracefully upon receiving signal 15
2025-04-18 12:14:01,575 INFO - Sending Signals.SIGTERM to group 656419. PIDs of all processes in the group: []
2025-04-18 12:14:01,575 INFO - Sending the signal Signals.SIGTERM to group 656419
2025-04-18 12:14:01,575 INFO - Sending the signal Signals.SIGTERM to process 656419 as process group is missing.
2025-04-18 12:14:01,576 INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
2025-04-18 12:14:01,639 INFO - Sending Signals.SIGTERM to group 656419. PIDs of all processes in the group: []
2025-04-18 12:14:01,640 INFO - Sending the signal Signals.SIGTERM to group 656419
2025-04-18 12:14:01,640 INFO - Sending the signal Signals.SIGTERM to process 656419 as process group is missing.
2025-04-18 12:14:01,641 INFO - Exited execute loop
2025-04-18 12:14:55,722 INFO - Loaded executor: LocalExecutor
2025-04-18 12:15:00,782 INFO - Starting the scheduler
2025-04-18 12:15:00,783 INFO - Processing each file at most -1 times
2025-04-18 12:15:00,911 INFO - Launched DagFileProcessorManager with pid: 657491
2025-04-18 12:15:00,914 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:15:00,920 INFO - Configured default timezone Asia/Seoul
2025-04-18 12:20:01,181 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:25:01,442 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:27:13,517 INFO - Exiting gracefully upon receiving signal 15
2025-04-18 12:27:13,616 INFO - Sending Signals.SIGTERM to group 657491. PIDs of all processes in the group: []
2025-04-18 12:27:13,616 INFO - Sending the signal Signals.SIGTERM to group 657491
2025-04-18 12:27:13,617 INFO - Sending the signal Signals.SIGTERM to process 657491 as process group is missing.
2025-04-18 12:27:13,617 INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
2025-04-18 12:27:13,681 INFO - Sending Signals.SIGTERM to group 657491. PIDs of all processes in the group: []
2025-04-18 12:27:13,682 INFO - Sending the signal Signals.SIGTERM to group 657491
2025-04-18 12:27:13,682 INFO - Sending the signal Signals.SIGTERM to process 657491 as process group is missing.
2025-04-18 12:27:13,682 INFO - Exited execute loop
2025-04-18 12:34:00,043 INFO - Loaded executor: LocalExecutor
2025-04-18 12:34:05,108 INFO - Starting the scheduler
2025-04-18 12:34:05,108 INFO - Processing each file at most -1 times
2025-04-18 12:34:05,280 INFO - Launched DagFileProcessorManager with pid: 667923
2025-04-18 12:34:05,283 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:34:05,290 INFO - Configured default timezone Asia/Seoul
2025-04-18 12:39:05,604 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:44:05,892 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:47:26,740 INFO - Exiting gracefully upon receiving signal 15
2025-04-18 12:47:26,816 INFO - Sending Signals.SIGTERM to group 667923. PIDs of all processes in the group: []
2025-04-18 12:47:26,817 INFO - Sending the signal Signals.SIGTERM to group 667923
2025-04-18 12:47:26,817 INFO - Sending the signal Signals.SIGTERM to process 667923 as process group is missing.
2025-04-18 12:47:26,817 INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
2025-04-18 12:47:26,882 INFO - Sending Signals.SIGTERM to group 667923. PIDs of all processes in the group: []
2025-04-18 12:47:26,882 INFO - Sending the signal Signals.SIGTERM to group 667923
2025-04-18 12:47:26,883 INFO - Sending the signal Signals.SIGTERM to process 667923 as process group is missing.
2025-04-18 12:47:26,883 INFO - Exited execute loop
2025-04-18 12:47:54,430 INFO - Loaded executor: LocalExecutor
2025-04-18 12:47:59,500 INFO - Starting the scheduler
2025-04-18 12:47:59,501 INFO - Processing each file at most -1 times
2025-04-18 12:47:59,627 INFO - Launched DagFileProcessorManager with pid: 673376
2025-04-18 12:47:59,629 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:47:59,636 INFO - Configured default timezone Asia/Seoul
2025-04-18 12:52:59,996 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-04-18 12:53:59,631 INFO - 1 tasks up for execution:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
2025-04-18 12:53:59,632 INFO - DAG dag_salesforce_to_s3_python_improved has 0/16 running and queued tasks
2025-04-18 12:53:59,632 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
2025-04-18 12:53:59,636 INFO - Trying to enqueue tasks: [<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:53:59,636 INFO - Sending TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 1 and queue default
2025-04-18 12:53:59,637 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
2025-04-18 12:53:59,698 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
2025-04-18 12:53:59,775 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_salesforce_to_s3_python_improved.py
2025-04-18 12:54:00,703 INFO - Running <TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:54:05,631 INFO - Marking run <DagRun dag_salesforce_to_s3_python_improved @ 2025-04-18 03:53:59.132483+00:00: manual__2025-04-18T03:53:59.132483+00:00, state:running, queued_at: 2025-04-18 03:53:59.170129+00:00. externally triggered: True> successful
2025-04-18 12:54:05,632 INFO - DagRun Finished: dag_id=dag_salesforce_to_s3_python_improved, execution_date=2025-04-18 03:53:59.132483+00:00, run_id=manual__2025-04-18T03:53:59.132483+00:00, run_start_date=2025-04-18 03:53:59.512868+00:00, run_end_date=2025-04-18 03:54:05.632608+00:00, run_duration=6.11974, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-18 03:53:59.132483+00:00, data_interval_end=2025-04-18 03:53:59.132483+00:00, dag_hash=a4fa51db3e4e05d07994e5ebb405e51c
2025-04-18 12:54:05,698 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=1, map_index=-1)
2025-04-18 12:54:05,706 INFO - TaskInstance Finished: dag_id=dag_salesforce_to_s3_python_improved, task_id=export_salesforce_to_s3_task, run_id=manual__2025-04-18T03:53:59.132483+00:00, map_index=-1, run_start_date=2025-04-18 03:54:00.992479+00:00, run_end_date=2025-04-18 03:54:04.445926+00:00, run_duration=3.453447, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-04-18 03:53:59.633407+00:00, queued_by_job_id=6, pid=675569
2025-04-18 12:54:31,078 INFO - Setting next_dagrun for dag_oracle_to_s3_python_improved to 2025-04-17 15:05:00+00:00, run_after=2025-04-18 15:05:00+00:00
2025-04-18 12:54:31,444 INFO - 2 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:54:31,445 INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
2025-04-18 12:54:31,445 INFO - DAG dag_oracle_to_s3_python_improved has 1/16 running and queued tasks
2025-04-18 12:54:31,445 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:54:31,448 INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>, <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:54:31,449 INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 2 and queue default
2025-04-18 12:54:31,449 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:31,449 INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 2 and queue default
2025-04-18 12:54:31,449 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:31,492 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:31,493 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:31,542 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
2025-04-18 12:54:31,581 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
2025-04-18 12:54:32,304 INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
2025-04-18 12:54:32,305 INFO - project name : dag_oracle_to_s3_python_improved
2025-04-18 12:54:32,305 INFO - project version : 0.1.0
2025-04-18 12:54:32,305 INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
2025-04-18 12:54:32,306 INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
2025-04-18 12:54:32,306 INFO - project start : 2025-04-18 12:54:32
2025-04-18 12:54:32,320 INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
2025-04-18 12:54:32,321 INFO - project name : dag_oracle_to_s3_python_improved
2025-04-18 12:54:32,321 INFO - project version : 0.1.0
2025-04-18 12:54:32,322 INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
2025-04-18 12:54:32,322 INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
2025-04-18 12:54:32,322 INFO - project start : 2025-04-18 12:54:32
2025-04-18 12:54:32,347 INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:54:32,367 INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task scheduled__2025-04-16T15:05:00+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:54:52,123 INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
2025-04-18 12:54:52,124 INFO - DAG dag_oracle_to_s3_python_improved has 1/16 running and queued tasks
2025-04-18 12:54:52,124 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>
2025-04-18 12:54:52,126 INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:54:52,127 INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 1 and queue default
2025-04-18 12:54:52,127 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:52,145 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1)
2025-04-18 12:54:52,145 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'scheduled__2025-04-16T15:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:52,150 INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=oralceExport_task, run_id=scheduled__2025-04-16T15:05:00+00:00, map_index=-1, run_start_date=2025-04-18 03:54:32.534117+00:00, run_end_date=2025-04-18 03:54:51.757934+00:00, run_duration=19.223817, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-04-18 03:54:31.446476+00:00, queued_by_job_id=6, pid=675838
2025-04-18 12:54:52,279 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
2025-04-18 12:54:53,000 INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
2025-04-18 12:54:53,001 INFO - project name : dag_oracle_to_s3_python_improved
2025-04-18 12:54:53,002 INFO - project version : 0.1.0
2025-04-18 12:54:53,002 INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
2025-04-18 12:54:53,003 INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
2025-04-18 12:54:53,003 INFO - project start : 2025-04-18 12:54:53
2025-04-18 12:54:53,051 INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task scheduled__2025-04-16T15:05:00+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:54:53,621 INFO - Marking run <DagRun dag_oracle_to_s3_python_improved @ 2025-04-16 15:05:00+00:00: scheduled__2025-04-16T15:05:00+00:00, state:running, queued_at: 2025-04-18 03:54:31.064965+00:00. externally triggered: False> successful
2025-04-18 12:54:53,621 INFO - DagRun Finished: dag_id=dag_oracle_to_s3_python_improved, execution_date=2025-04-16 15:05:00+00:00, run_id=scheduled__2025-04-16T15:05:00+00:00, run_start_date=2025-04-18 03:54:31.099665+00:00, run_end_date=2025-04-18 03:54:53.621738+00:00, run_duration=22.522073, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-04-16 15:05:00+00:00, data_interval_end=2025-04-17 15:05:00+00:00, dag_hash=1468a60cd25d7fdf91c8485574c935b5
2025-04-18 12:54:53,629 INFO - Setting next_dagrun for dag_oracle_to_s3_python_improved to 2025-04-17 15:05:00+00:00, run_after=2025-04-18 15:05:00+00:00
2025-04-18 12:54:53,753 INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:54:53,754 INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
2025-04-18 12:54:53,755 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:54:53,758 INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:54:53,759 INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1) to LocalExecutor with priority 1 and queue default
2025-04-18 12:54:53,759 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:53,773 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1)
2025-04-18 12:54:53,773 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='scheduled__2025-04-16T15:05:00+00:00', try_number=1, map_index=-1)
2025-04-18 12:54:53,773 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:54:53,782 INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=oralceExport_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:54:32.498437+00:00, run_end_date=2025-04-18 03:54:52.828031+00:00, run_duration=20.329594, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-04-18 03:54:31.446476+00:00, queued_by_job_id=6, pid=675836
2025-04-18 12:54:53,783 INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=cleanSpace_task, run_id=scheduled__2025-04-16T15:05:00+00:00, map_index=-1, run_start_date=2025-04-18 03:54:53.153089+00:00, run_end_date=2025-04-18 03:54:53.440555+00:00, run_duration=0.287466, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-18 03:54:52.125151+00:00, queued_by_job_id=6, pid=676021
2025-04-18 12:54:53,876 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
2025-04-18 12:54:54,572 INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
2025-04-18 12:54:54,573 INFO - project name : dag_oracle_to_s3_python_improved
2025-04-18 12:54:54,573 INFO - project version : 0.1.0
2025-04-18 12:54:54,573 INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
2025-04-18 12:54:54,573 INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
2025-04-18 12:54:54,574 INFO - project start : 2025-04-18 12:54:54
2025-04-18 12:54:54,616 INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:54:56,053 INFO - Marking run <DagRun dag_oracle_to_s3_python_improved @ 2025-04-18 03:54:30.166214+00:00: manual__2025-04-18T03:54:30.166214+00:00, state:running, queued_at: 2025-04-18 03:54:30.206488+00:00. externally triggered: True> successful
2025-04-18 12:54:56,054 INFO - DagRun Finished: dag_id=dag_oracle_to_s3_python_improved, execution_date=2025-04-18 03:54:30.166214+00:00, run_id=manual__2025-04-18T03:54:30.166214+00:00, run_start_date=2025-04-18 03:54:31.100184+00:00, run_end_date=2025-04-18 03:54:56.054231+00:00, run_duration=24.954047, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-16 15:05:00+00:00, data_interval_end=2025-04-17 15:05:00+00:00, dag_hash=1468a60cd25d7fdf91c8485574c935b5
2025-04-18 12:54:56,110 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=1, map_index=-1)
2025-04-18 12:54:56,116 INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=cleanSpace_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:54:54.707455+00:00, run_end_date=2025-04-18 03:54:55.031334+00:00, run_duration=0.323879, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-18 03:54:53.756307+00:00, queued_by_job_id=6, pid=676048
2025-04-18 12:55:29,578 INFO - 1 tasks up for execution:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
2025-04-18 12:55:29,578 INFO - DAG dag_salesforce_to_s3_python_improved has 0/16 running and queued tasks
2025-04-18 12:55:29,579 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>
2025-04-18 12:55:29,581 INFO - Trying to enqueue tasks: [<TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:55:29,581 INFO - Sending TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=2, map_index=-1) to LocalExecutor with priority 1 and queue default
2025-04-18 12:55:29,581 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
2025-04-18 12:55:29,585 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_salesforce_to_s3_python_improved', 'export_salesforce_to_s3_task', 'manual__2025-04-18T03:53:59.132483+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_salesforce_to_s3_python_improved.py']
2025-04-18 12:55:29,617 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_salesforce_to_s3_python_improved.py
2025-04-18 12:55:30,434 INFO - Running <TaskInstance: dag_salesforce_to_s3_python_improved.export_salesforce_to_s3_task manual__2025-04-18T03:53:59.132483+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:55:33,844 INFO - Marking run <DagRun dag_salesforce_to_s3_python_improved @ 2025-04-18 03:53:59.132483+00:00: manual__2025-04-18T03:53:59.132483+00:00, state:running, queued_at: 2025-04-18 03:55:28.598355+00:00. externally triggered: True> successful
2025-04-18 12:55:33,845 INFO - DagRun Finished: dag_id=dag_salesforce_to_s3_python_improved, execution_date=2025-04-18 03:53:59.132483+00:00, run_id=manual__2025-04-18T03:53:59.132483+00:00, run_start_date=2025-04-18 03:55:29.548043+00:00, run_end_date=2025-04-18 03:55:33.845339+00:00, run_duration=4.297296, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-18 03:53:59.132483+00:00, data_interval_end=2025-04-18 03:53:59.132483+00:00, dag_hash=a4fa51db3e4e05d07994e5ebb405e51c
2025-04-18 12:55:33,859 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_salesforce_to_s3_python_improved', task_id='export_salesforce_to_s3_task', run_id='manual__2025-04-18T03:53:59.132483+00:00', try_number=2, map_index=-1)
2025-04-18 12:55:33,864 INFO - TaskInstance Finished: dag_id=dag_salesforce_to_s3_python_improved, task_id=export_salesforce_to_s3_task, run_id=manual__2025-04-18T03:53:59.132483+00:00, map_index=-1, run_start_date=2025-04-18 03:55:30.531349+00:00, run_end_date=2025-04-18 03:55:33.361262+00:00, run_duration=2.829913, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-04-18 03:55:29.579751+00:00, queued_by_job_id=6, pid=676307
2025-04-18 12:55:40,592 INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:55:40,592 INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
2025-04-18 12:55:40,592 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:55:40,594 INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:55:40,595 INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1) to LocalExecutor with priority 2 and queue default
2025-04-18 12:55:40,595 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:55:40,599 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'oralceExport_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:55:40,627 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
2025-04-18 12:55:41,248 INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
2025-04-18 12:55:41,249 INFO - project name : dag_oracle_to_s3_python_improved
2025-04-18 12:55:41,249 INFO - project version : 0.1.0
2025-04-18 12:55:41,250 INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
2025-04-18 12:55:41,250 INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
2025-04-18 12:55:41,251 INFO - project start : 2025-04-18 12:55:41
2025-04-18 12:55:41,297 INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.oralceExport_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:55:52,924 INFO - 1 tasks up for execution:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:55:52,924 INFO - DAG dag_oracle_to_s3_python_improved has 0/16 running and queued tasks
2025-04-18 12:55:52,925 INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>
2025-04-18 12:55:52,927 INFO - Trying to enqueue tasks: [<TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [scheduled]>] for executor: LocalExecutor(parallelism=32)
2025-04-18 12:55:52,927 INFO - Sending TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1) to LocalExecutor with priority 1 and queue default
2025-04-18 12:55:52,928 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:55:52,931 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='oralceExport_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1)
2025-04-18 12:55:52,932 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dag_oracle_to_s3_python_improved', 'cleanSpace_task', 'manual__2025-04-18T03:54:30.166214+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_oracle_to_s3_python_improved.py']
2025-04-18 12:55:52,937 INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=oralceExport_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:55:41.389328+00:00, run_end_date=2025-04-18 03:55:52.563060+00:00, run_duration=11.173732, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-04-18 03:55:40.593298+00:00, queued_by_job_id=6, pid=676396
2025-04-18 12:55:52,963 INFO - Filling up the DagBag from /home/kim/airflow_tutorial/dags/dag_oracle_to_s3_python_improved.py
2025-04-18 12:55:53,709 INFO - load config file  : /home/kim/airflow_tutorial/dags/resource/dag_with_rdbms_tar_ETL_config.yaml 
2025-04-18 12:55:53,710 INFO - project name : dag_oracle_to_s3_python_improved
2025-04-18 12:55:53,710 INFO - project version : 0.1.0
2025-04-18 12:55:53,710 INFO - project description : (EL) locknlock oracle tables (2025) > locknlock_s3 json.gz files
2025-04-18 12:55:53,711 INFO - project tags : ['prd', 'migration', 'oralce', 's3', 'python']
2025-04-18 12:55:53,711 INFO - project start : 2025-04-18 12:55:53
2025-04-18 12:55:53,753 INFO - Running <TaskInstance: dag_oracle_to_s3_python_improved.cleanSpace_task manual__2025-04-18T03:54:30.166214+00:00 [queued]> on host DESKTOP-C0R54F3.
2025-04-18 12:55:55,100 INFO - Marking run <DagRun dag_oracle_to_s3_python_improved @ 2025-04-18 03:54:30.166214+00:00: manual__2025-04-18T03:54:30.166214+00:00, state:running, queued_at: 2025-04-18 03:55:38.701679+00:00. externally triggered: True> successful
2025-04-18 12:55:55,100 INFO - DagRun Finished: dag_id=dag_oracle_to_s3_python_improved, execution_date=2025-04-18 03:54:30.166214+00:00, run_id=manual__2025-04-18T03:54:30.166214+00:00, run_start_date=2025-04-18 03:55:39.687016+00:00, run_end_date=2025-04-18 03:55:55.100801+00:00, run_duration=15.413785, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-04-16 15:05:00+00:00, data_interval_end=2025-04-17 15:05:00+00:00, dag_hash=1468a60cd25d7fdf91c8485574c935b5
2025-04-18 12:55:55,116 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_oracle_to_s3_python_improved', task_id='cleanSpace_task', run_id='manual__2025-04-18T03:54:30.166214+00:00', try_number=2, map_index=-1)
2025-04-18 12:55:55,120 INFO - TaskInstance Finished: dag_id=dag_oracle_to_s3_python_improved, task_id=cleanSpace_task, run_id=manual__2025-04-18T03:54:30.166214+00:00, map_index=-1, run_start_date=2025-04-18 03:55:53.836780+00:00, run_end_date=2025-04-18 03:55:54.042132+00:00, run_duration=0.205352, state=success, executor=LocalExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-04-18 03:55:52.925594+00:00, queued_by_job_id=6, pid=676532
2025-04-18 12:58:00,365 INFO - Adopting or resetting orphaned tasks for active dag runs
